import OpenAI from 'openai'
import { CONFIG } from '../../config/constants.js'
import dotenv from 'dotenv'
import { ResponseCache } from '../cache/response-cache.js'

dotenv.config()

if (!process.env.OPENAI_API_KEY) {
    throw new Error('OPENAI_API_KEY no estÃ¡ configurada en el archivo .env')
}

const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
})

const MAX_HISTORY_MESSAGES = 10 // Limitar historial a los Ãºltimos 5 mensajes

export const OpenAIService = {
    async generateChatResponse(messages, behaviorPrompt = '', knowledgePrompt = '', isAudioResponse = false, chatbotId = null) {
        try {
            console.log('ðŸ¤– OpenAI: Preparando mensajes para generar respuesta')
            
            // Verificar cachÃ© si tenemos chatbotId y el Ãºltimo mensaje
            if (chatbotId && messages.length > 0) {
                const lastMessage = messages[messages.length - 1]
                if (lastMessage.role === 'user') {
                    const cachedResponse = await ResponseCache.get(chatbotId, lastMessage.content)
                    if (cachedResponse) {
                        console.log('ðŸ¤– OpenAI: Respuesta encontrada en cachÃ©')
                        return cachedResponse
                    }
                }
            }

            // Limitar el historial a los Ãºltimos mensajes
            const limitedMessages = messages.slice(-MAX_HISTORY_MESSAGES)
            
            // Preparar mensajes del sistema
            const systemMessages = []
            
            if (behaviorPrompt) {
                console.log('ðŸ¤– OpenAI: Agregando prompt de comportamiento')
                systemMessages.push({
                    role: 'system',
                    content: behaviorPrompt
                })
            }
            
            if (knowledgePrompt) {
                console.log('ðŸ¤– OpenAI: Agregando prompt de conocimiento')
                systemMessages.push({
                    role: 'system',
                    content: knowledgePrompt
                })
            }

            // Combinar mensajes
            const fullMessages = [
                ...systemMessages,
                ...limitedMessages
            ]

            console.log('ðŸ¤– OpenAI: Total de mensajes:', fullMessages.length)

            // ConfiguraciÃ³n especÃ­fica segÃºn el tipo de respuesta "gpt-4o"
            const config = {
                model: "gpt-4o",
                messages: fullMessages,
                temperature: 0.5,
                max_tokens: 100,
                presence_penalty: 0.3,
                frequency_penalty: 0.3,
                top_p: 0.9,
                stop: null
            }

            // Ajustar configuraciÃ³n para respuestas de audio
            if (isAudioResponse) {
                config.temperature = 0.5
                config.max_tokens = 100
                config.presence_penalty = 0.3
                config.frequency_penalty = 0.3
                config.top_p = 0.9
                
                // Agregar instrucciÃ³n especÃ­fica para respuestas de audio
                config.messages.unshift({
                    role: 'system',
                    content: 'Proporciona respuestas completas y concisas. Si mencionas que darÃ¡s informaciÃ³n, inclÃºyela en el mismo mensaje. Evita frases como "a continuaciÃ³n" o "te proporcionarÃ©" sin dar la informaciÃ³n. no envies emojis o emoticones.'
                })
            }

            console.log('ðŸ¤– OpenAI: ConfiguraciÃ³n:', {
                isAudio: isAudioResponse,
                maxTokens: config.max_tokens,
                temperature: config.temperature
            })

            // Generar respuesta
            console.log('ðŸ¤– OpenAI: Llamando a la API...')
            const completion = await openai.chat.completions.create(config)

            const response = completion.choices[0].message.content
            console.log('ðŸ¤– OpenAI: Respuesta generada:', response.substring(0, 50) + '...')

            // Guardar en cachÃ© si tenemos chatbotId y mensaje del usuario
            if (chatbotId && messages.length > 0) {
                const lastMessage = messages[messages.length - 1]
                if (lastMessage.role === 'user') {
                    await ResponseCache.set(chatbotId, lastMessage.content, response)
                }
            }

            return response
        } catch (error) {
            console.error('ðŸ¤– OpenAI Error:', {
                message: error.message,
                type: error.type,
                code: error.code
            })
            throw new Error('Error generando respuesta de IA: ' + error.message)
        }
    },

    async generateEmbedding(text) {
        try {
            console.log('ðŸ¤– OpenAI: Generando embedding para texto')
            const response = await openai.embeddings.create({
                model: "text-embedding-ada-002",
                input: text
            })

            console.log('ðŸ¤– OpenAI: Embedding generado exitosamente')
            return response.data[0].embedding
        } catch (error) {
            console.error('ðŸ¤– OpenAI Embedding Error:', error)
            throw new Error('Error generando embedding: ' + error.message)
        }
    },

    async isResponseRelevant(question, answer, threshold = 0.6) {
        try {
            console.log('ðŸ¤– OpenAI: Verificando relevancia de respuesta')
            
            // Normalizar textos para comparaciÃ³n
            const normalizedQuestion = question.toLowerCase().trim()
            const normalizedAnswer = answer.toLowerCase().trim()

            // Verificar si la respuesta es demasiado genÃ©rica
            const genericPhrases = [
                'no puedo proporcionar',
                'no tengo informaciÃ³n',
                'no estoy seguro',
                'no puedo ayudar'
            ]
            
            if (genericPhrases.some(phrase => normalizedAnswer.includes(phrase))) {
                console.log('ðŸ¤– OpenAI: Respuesta demasiado genÃ©rica')
                return false
            }

            // Generar embeddings
            const [questionEmbedding, answerEmbedding] = await Promise.all([
                this.generateEmbedding(question),
                this.generateEmbedding(answer)
            ])

            // Calcular similitud
            const similarity = this.cosineSimilarity(questionEmbedding, answerEmbedding)
            console.log('ðŸ¤– OpenAI: Similitud calculada:', similarity)
            
            // Verificar longitud mÃ­nima de respuesta
            if (answer.length < 20) {
                console.log('ðŸ¤– OpenAI: Respuesta demasiado corta')
                return false
            }

            return similarity >= threshold
        } catch (error) {
            console.error('ðŸ¤– OpenAI Relevance Error:', error)
            return false
        }
    },

    cosineSimilarity(vecA, vecB) {
        const dotProduct = vecA.reduce((acc, val, i) => acc + val * vecB[i], 0)
        const normA = Math.sqrt(vecA.reduce((acc, val) => acc + val * val, 0))
        const normB = Math.sqrt(vecB.reduce((acc, val) => acc + val * val, 0))
        return dotProduct / (normA * normB)
    }
}